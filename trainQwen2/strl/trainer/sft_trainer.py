# Copyright 2025 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import dataclasses
import os
import warnings
from collections import defaultdict
from typing import Any, Callable, Optional, Type, Union

import torch
import torch.nn as nn
import transformers
from accelerate import PartialState
from datasets import Dataset, IterableDataset
from packaging import version
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BaseImageProcessor,
    DataCollator,
    DataCollatorForLanguageModeling,
    FeatureExtractionMixin,
    PreTrainedModel,
    # PreTrainedTokenizerBase,
    ProcessorMixin,
    # Trainer,
    TrainingArguments,
    is_wandb_available,
)
from llama.tokenization_llama_fast import LlamaTokenizerFast
from tokenization_utils_base import PreTrainedTokenizerBase
from trainer import Trainer
from transformers.trainer_callback import TrainerCallback
from transformers.trainer_utils import EvalPrediction
from transformers.utils import is_liger_kernel_available, is_peft_available
from transformers.utils.deprecation import deprecate_kwarg

from ..data_utils import is_conversational, maybe_apply_chat_template, maybe_convert_to_chatml, pack_examples
from .sft_config import SFTConfig
from .utils import ConstantLengthDataset, generate_model_card, get_comet_experiment_url, peft_module_casting_to_bf16


if is_peft_available():
    import speft
    from speft import PeftConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training

if is_liger_kernel_available():
    from liger_kernel.transformers import AutoLigerKernelForCausalLM

if is_wandb_available():
    import wandb

bPrintMoreKv = True
class SFTTrainer(Trainer):
    """
    Trainer for Supervised Fine-Tuning (SFT) method.

    This class is a wrapper around the [`transformers.Trainer`] class and inherits all of its attributes and methods.

    Example:

    ```python
    from datasets import load_dataset
    from trl import SFTTrainer

    dataset = load_dataset("roneneldan/TinyStories", split="train[:1%]")

    trainer = SFTTrainer(model="Qwen/Qwen2-0.5B-Instruct", train_dataset=dataset)
    trainer.train()
    ```

    Args:
        model (`Union[str, PreTrainedModel]`):
            Model to be trained. Can be either:

            - A string, being the *model id* of a pretrained model hosted inside a model repo on huggingface.co, or
              a path to a *directory* containing model weights saved using
              [`~transformers.PreTrainedModel.save_pretrained`], e.g., `'./my_model_directory/'`. The model is
              loaded using [`~transformers.AutoModelForCausalLM.from_pretrained`] with the keywork arguments
              in `args.model_init_kwargs`.
            - A [`~transformers.PreTrainedModel`] object. Only causal language models are supported.
        args ([`SFTConfig`], *optional*, defaults to `None`):
            Configuration for this trainer. If `None`, a default configuration is used.
        data_collator (`DataCollator`, *optional*):
            Function to use to form a batch from a list of elements of the prcessed `train_dataset` or `eval_dataset`.
            Will default to [`~transformers.default_data_collator`] if no `processing_class` is provided, an instance
            of [`~transformers.DataCollatorWithPadding`] otherwise if the processing_class is a feature extractor or
            tokenizer.
        train_dataset ([`~datasets.Dataset`] or [`~datasets.IterableDataset`]):
            Dataset to use for training. SFT supports both [language modeling](#language-modeling) type and
            [prompt-completion](#prompt-completion) type. The format of the samples can be either:

            - [Standard](dataset_formats#standard): Each sample contains plain text.
            - [Conversational](dataset_formats#conversational): Each sample contains structured messages (e.g., role
              and content).

            The trainer also supports processed datasets (tokenized) as long as they contain an `input_ids` field.
        eval_dataset ([`~datasets.Dataset`], [`~datasets.IterableDataset`] or `dict[str, Union[Dataset, IterableDataset]]`):
            Dataset to use for evaluation. It must meet the same requirements as `train_dataset`.
        processing_class ([`~transformers.PreTrainedTokenizerBase`], *optional*, defaults to `None`):
            Processing class used to process the data. If `None`, the processing class is loaded from the model's name
            with [`~transformers.AutoTokenizer.from_pretrained`].
        callbacks (list of [`~transformers.TrainerCallback`], *optional*, defaults to `None`):
            List of callbacks to customize the training loop. Will add those to the list of default callbacks
            detailed in [here](https://huggingface.co/docs/transformers/main_classes/callback).

            If you want to remove one of the default callbacks used, use the [`~transformers.Trainer.remove_callback`]
            method.
        optimizers (`tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*, defaults to `(None, None)`):
            A tuple containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your
            model and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.
        optimizer_cls_and_kwargs (`Tuple[Type[torch.optim.Optimizer], Dict[str, Any]]`, *optional*, defaults to `None`):
            A tuple containing the optimizer class and keyword arguments to use.
            Overrides `optim` and `optim_args` in `args`. Incompatible with the `optimizers` argument.

            Unlike `optimizers`, this argument avoids the need to place model parameters on the correct devices before initializing the Trainer.
        preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*, defaults to `None`):
            A function that preprocess the logits right before caching them at each evaluation step. Must take two
            tensors, the logits and the labels, and return the logits once processed as desired. The modifications made
            by this function will be reflected in the predictions received by `compute_metrics`.

            Note that the labels (second parameter) will be `None` if the dataset does not have them.
        peft_config ([`~peft.PeftConfig`], *optional*, defaults to `None`):
            PEFT configuration used to wrap the model. If `None`, the model is not wrapped.
        formatting_func (`Optional[Callable]`):
            Formatting function applied to the dataset before tokenization.
    """

    _tag_names = ["trl", "sft"]

    @deprecate_kwarg(
        "tokenizer", "0.16.0", "processing_class", warn_if_greater_or_equal_version=True, raise_if_both_names=True
    )
    def __init__(
        self,
        model: Union[str, nn.Module, PreTrainedModel],
        args: Optional[Union[SFTConfig, TrainingArguments]] = None,
        data_collator: Optional[DataCollator] = None,  # type: ignore
        train_dataset: Optional[Union[Dataset, IterableDataset]] = None,
        eval_dataset: Optional[Union[Dataset, dict[str, Dataset]]] = None,
        processing_class: Optional[
            Union[PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin]
        ] = None,
        compute_loss_func: Optional[Callable] = None,
        compute_metrics: Optional[Callable[[EvalPrediction], dict]] = None,
        callbacks: Optional[list[TrainerCallback]] = None,
        optimizers: tuple[Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None),
        optimizer_cls_and_kwargs: Optional[tuple[Type[torch.optim.Optimizer], dict[str, Any]]] = None,
        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,
        peft_config: Optional["PeftConfig"] = None,
        formatting_func: Optional[Union[Callable[[dict], str], Callable[[dict], list[str]]]] = None,
    ):
        print("\n" + "="*40)
        print("Initializing SFT Trainer".center(40))
        print("="*40)
        
        # Args initialization
        print("\n[Phase 1] å‚æ•°åˆå§‹åŒ–".ljust(40, '-'))
        if args is None:
            model_name = model if isinstance(model, str) else model.config._name_or_path
            short_name = model_name.split("/")[-1]
            args = SFTConfig(f"{short_name}-SFT")
            print(f"è‡ªåŠ¨ç”ŸæˆSFTé…ç½® | æ¨¡å‹ç®€ç§°: {short_name} | é…ç½®åç§°: {args.output_dir}")
        elif isinstance(args, TrainingArguments) and not isinstance(args, SFTConfig):
            dict_args = args.to_dict()
            dict_args["hub_token"] = args.hub_token  # to_dict hides the hub_token
            dict_args.pop("push_to_hub_token")
            args = SFTConfig(**dict_args)
            print(f"è½¬æ¢TrainingArgumentsåˆ°SFTConfig | åŸå§‹å‚æ•°æ•°é‡: {len(dict_args)} | è½¬æ¢åå‚æ•°ç¤ºä¾‹: {list(dict_args.keys())[:3]}...")
        
        # Model initialization
        print("\n[Phase 2] æ¨¡å‹åˆå§‹åŒ–".ljust(40, '-'))
        if args.model_init_kwargs is not None and not isinstance(model, str):
            warnings.warn("model_init_kwargså°†è¢«å¿½ç•¥ï¼ˆæ¨¡å‹å·²å®ä¾‹åŒ–ï¼‰")
            print(f"âš ï¸ è­¦å‘Š | å¿½ç•¥model_init_kwargså‚æ•°: {list(args.model_init_kwargs.keys())}")
            
        if isinstance(model, str):
            print(f"ä»è·¯å¾„åŠ è½½æ¨¡å‹ | æ¨¡å‹æ ‡è¯†: {model}")
            model = self._create_model_from_path(model, args)
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ | ç±»å‹: {type(model).__name__} | å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        else:
            print(f"ä½¿ç”¨å·²æœ‰æ¨¡å‹å®ä¾‹ | ç±»å‹: {type(model).__name__} | æ˜¯å¦å†»ç»“å‚æ•°: {all(not p.requires_grad for p in model.parameters())}")
        
        # PEFT configuration
        if peft_config is not None:
            print("\n[Phase 3] PEFTé…ç½®".ljust(40, '-'))
            print(f"åº”ç”¨PEFTé…ç½®å‰æ¨¡å‹ç»“æ„: {model}")
            print(f"PEFTé…ç½®è¯¦æƒ…: {peft_config.to_dict()}")
            model = self._prepare_peft_model(model, peft_config, args)
            # print(f"âœ… PEFTåº”ç”¨å®Œæˆ | æ–°æ¨¡å‹ç»“æ„: {type(model).__name__} | å¯è®­ç»ƒå‚æ•°å æ¯”: {self._get_trainable_parameter_ratio(model):.1%}")
        
        # Tokenizer handling
        print("\n[Phase 4] åˆ†è¯å™¨å¤„ç†".ljust(40, '-'))
        if processing_class is None:
            model_path = model.config._name_or_path if hasattr(model, 'config') else 'Unknown'
            print(f"è‡ªåŠ¨åŠ è½½åˆ†è¯å™¨ | æ¨¡å‹è·¯å¾„: {model_path}")
            processing_class = LlamaTokenizerFast.from_pretrained(model_path)
            if processing_class.pad_token is None:
                processing_class.pad_token = processing_class.eos_token
                print(f"âš ï¸ è®¾ç½®pad_tokenä¸ºeos_token | pad_token_id: {processing_class.pad_token_id}")
            print(f"åˆ†è¯å™¨ç±»å‹: {type(processing_class).__name__} | è¯æ±‡é‡: {processing_class.vocab_size:,}")
        else:
            print(f"ä½¿ç”¨è‡ªå®šä¹‰å¤„ç†å™¨ | ç±»å‹: {type(processing_class).__name__}")
        
        # Dataset preparation
        print("\n[Phase 5] æ•°æ®é›†å‡†å¤‡".ljust(40, '-'))
        preprocess_dataset = args.dataset_kwargs is None or not args.dataset_kwargs.get("skip_prepare_dataset", False)

        # æ•°æ®é›†é¢„å¤„ç†é€»è¾‘
        print("\n[Phase 6] æ•°æ®é›†é¢„å¤„ç†".ljust(40, '-'))
        if preprocess_dataset:
            print(f"å¼€å§‹è®­ç»ƒé›†é¢„å¤„ç† | åŸå§‹æ•°æ®é›†ç±»å‹: {type(train_dataset).__name__}")
            original_train_size = len(train_dataset) if hasattr(train_dataset, '__len__') else 'Unknown'
            
            # è¿›è¡Œæ•°æ®é¢„å¤„ç†
            train_dataset = self._prepare_dataset(
                train_dataset, processing_class, args, args.packing, formatting_func, "train"
            )
            
            new_train_size = len(train_dataset) if hasattr(train_dataset, '__len__') else 'Unknown'
            print(f"âœ… è®­ç»ƒé›†é¢„å¤„ç†å®Œæˆ | å¤„ç†å‰æ ·æœ¬æ•°: {original_train_size} -> å¤„ç†å: {new_train_size}")
            
            if eval_dataset is not None:
                packing = args.packing if args.eval_packing is None else args.eval_packing
                print(f"\nè¯„ä¼°é›†æ‰“åŒ…ç­–ç•¥: {'å¯ç”¨' if packing else 'ç¦ç”¨'}")
                
                if isinstance(eval_dataset, dict):
                    print(f"å¤„ç†å­—å…¸æ ¼å¼è¯„ä¼°é›† | åŒ…å«{len(eval_dataset)}ä¸ªå­é›†: {list(eval_dataset.keys())}")
                    for key in eval_dataset:
                        original_eval_size = len(eval_dataset[key]) if hasattr(eval_dataset[key], '__len__') else 'Unknown'
                        eval_dataset[key] = self._prepare_dataset(
                            eval_dataset[key], processing_class, args, packing, formatting_func, key
                        )
                        new_eval_size = len(eval_dataset[key]) if hasattr(eval_dataset[key], '__len__') else 'Unknown'
                        print(f"  {key}å­é›†å¤„ç†å®Œæˆ | {original_eval_size} -> {new_eval_size}")
                else:
                    original_eval_size = len(eval_dataset) if hasattr(eval_dataset, '__len__') else 'Unknown'
                    eval_dataset = self._prepare_dataset(
                        eval_dataset, processing_class, args, packing, formatting_func, "eval"
                    )
                    new_eval_size = len(eval_dataset) if hasattr(eval_dataset, '__len__') else 'Unknown'
                    print(f"è¯„ä¼°é›†å¤„ç†å®Œæˆ | {original_eval_size} -> {new_eval_size}")
        else:
            print("âš ï¸ è·³è¿‡æ•°æ®é›†é¢„å¤„ç†æ­¥éª¤")

        # æ•°æ®æ•´ç†å™¨åˆå§‹åŒ–
        print("\n[Phase 7] æ•°æ®æ•´ç†å™¨é…ç½®".ljust(40, '-'))
        if data_collator is None:
            print("è‡ªåŠ¨åˆ›å»ºé»˜è®¤æ•°æ®æ•´ç†å™¨")
            print(f"åˆ†è¯å™¨ç±»å‹: {type(processing_class).__name__} | MLM: {'å¦'}")
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=processing_class, 
                mlm=False
            )
            print(f"âœ… æ•°æ®æ•´ç†å™¨åˆ›å»ºå®Œæˆ | ç±»å‹: {type(data_collator).__name__}")
        else:
            print(f"ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®æ•´ç†å™¨ | ç±»å‹: {type(data_collator).__name__}")

        self._metrics = defaultdict(list)

        # çˆ¶ç±»åˆå§‹åŒ–å…¼å®¹æ€§å¤„ç†
        print("\n[Phase 8] çˆ¶ç±»åˆå§‹åŒ–å‡†å¤‡".ljust(40, '-'))
        super_init_kwargs = {}
        transformers_version = version.parse(transformers.__version__)
        required_version = version.parse("4.47.0.dev0")
        
        print(f"æ£€æµ‹Transformersç‰ˆæœ¬: {transformers_version}")
        if transformers_version >= required_version:
            print(f"ç‰ˆæœ¬å…¼å®¹ (â‰¥{required_version}) | æ³¨å…¥ä¼˜åŒ–å™¨é…ç½®")
            super_init_kwargs["optimizer_cls_and_kwargs"] = optimizer_cls_and_kwargs
            if optimizer_cls_and_kwargs:
                opt_cls, opt_kwargs = optimizer_cls_and_kwargs
                print(f"è‡ªå®šä¹‰ä¼˜åŒ–å™¨é…ç½®: {opt_cls.__name__} | å‚æ•°ç¤ºä¾‹: {list(opt_kwargs.keys())[:2]}")
        else:
            if optimizer_cls_and_kwargs is not None:
                warnings.warn("æ—§ç‰ˆæœ¬Transformersä¸æ”¯æŒoptimizer_cls_and_kwargs")
                print(f"âš ï¸ ç‰ˆæœ¬è¿‡ä½ ({transformers_version} < {required_version}) | å·²å¿½ç•¥ä¼˜åŒ–å™¨é…ç½®")

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        print("\n[Phase 9] æ‰§è¡Œçˆ¶ç±»åˆå§‹åŒ–".ljust(40, '-'))
        print("ä¼ é€’å…³é”®å‚æ•°åˆ—è¡¨:")
        print(f"  - æ¨¡å‹ç±»å‹: {type(model).__name__}")
        print(f"  - è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_dataset) if train_dataset else 'None'}")
        print(f"  - å›è°ƒå‡½æ•°æ•°é‡: {len(callbacks) if callbacks else 0}")
        
        print("è°ƒç”¨super().__init__...")
        super().__init__(
            model=model,
            args=args,
            data_collator=data_collator,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            processing_class=processing_class,
            compute_loss_func=compute_loss_func,
            compute_metrics=compute_metrics,
            callbacks=callbacks,
            optimizers=optimizers,
            preprocess_logits_for_metrics=preprocess_logits_for_metrics,
            **super_init_kwargs,
        )
        print("âœ… çˆ¶ç±»åˆå§‹åŒ–å®Œæˆ")

        # æ¨¡å‹æ ‡ç­¾å¤„ç†
        print("\n[Phase 10] æ¨¡å‹å…ƒæ•°æ®é…ç½®".ljust(40, '-'))
        print(f"å½“å‰æ¨¡å‹æ ‡ç­¾: {getattr(self, '_tag_names', 'æœªå®šä¹‰')}")
        if hasattr(self.model, "add_model_tags"):
            print("æ£€æµ‹åˆ°æ¨¡å‹æ ‡ç­¾æ¥å£ï¼Œæ·»åŠ è®­ç»ƒæ–¹æ³•æ ‡ç­¾")
            # self.model.add_model_tag("sft")
            # self.model.add_model_tag("transformers_trainer")
            self.model.add_model_tags(self._tag_names)
            print(f"æ›´æ–°åæ ‡ç­¾: {self.model.__class__.__name__} çš„æ ‡ç­¾åˆ—è¡¨")
        else:
            print("âš ï¸ æ¨¡å‹ä¸æ”¯æŒæ ‡ç­¾æ·»åŠ åŠŸèƒ½")


    def _create_model_from_path(self, model_path: str, args: SFTConfig) -> PreTrainedModel:
        """Creates a model from a path or model identifier."""
        model_init_kwargs = args.model_init_kwargs or {}
        # Handle torch dtype
        torch_dtype = model_init_kwargs.get("torch_dtype")
        if isinstance(torch_dtype, torch.dtype) or torch_dtype == "auto" or torch_dtype is None:
            pass  # torch_dtype is already a torch.dtype or "auto" or None
        elif isinstance(torch_dtype, str):  # it's a str, but not "auto"
            torch_dtype = getattr(torch, torch_dtype)
            model_init_kwargs["torch_dtype"] = torch_dtype
        else:
            raise ValueError(
                "Invalid `torch_dtype` passed to `SFTConfig`. Expected either 'auto' or a string representing "
                f"a `torch.dtype` (e.g., 'float32'), but got {torch_dtype}."
            )
        # Disable caching if gradient checkpointing is enabled (not supported)
        if args.gradient_checkpointing:
            model_init_kwargs["use_cache"] = False

        # Create model
        if args.use_liger:
            if not is_liger_kernel_available():
                raise ImportError("Please install Liger-kernel for use_liger=True")
            model = AutoLigerKernelForCausalLM.from_pretrained(model_path, **model_init_kwargs)
        else:
            model = AutoModelForCausalLM.from_pretrained(model_path, **model_init_kwargs)
        return model

    def _prepare_peft_model(self, model: PreTrainedModel, peft_config: Any, args: SFTConfig) -> PreTrainedModel:
        """Prepares a model for PEFT training."""
        if not is_peft_available():
            raise ImportError("To use PeftModel, you need to install the `peft` library.")

        if not isinstance(peft_config, PeftConfig):
            print(" ====æœŸæœ›çš„ç±»å‹==== ", PeftConfig)
            raise ValueError(
                f"Expected PeftConfig object but got {type(peft_config)}. If you want to use the PeftModel, you need "
                "to pass a PeftConfig object to the SFTTrainer."
            )

        if isinstance(model, PeftModel):
            return model

        # Handle quantized models (QLoRA)
        # æ˜¯å¦è¿›è¡Œä½ç²¾åº¦è®­ç»ƒ
        is_qlora = getattr(model, "is_loaded_in_4bit", False) or getattr(model, "is_loaded_in_8bit", False)

        is_sharded_qlora = False
        if getattr(model, "is_loaded_in_4bit", False):
            # Check if model is sharded (FSDP/DS-Zero3)
            for _, param in model.named_parameters():
                if param.__class__.__name__ == "Params4bit":
                    is_sharded_qlora = param.data.device.type in {"cpu", "meta"}
                    break

        # Prepare model for kbit training if needed
        if is_qlora and not is_sharded_qlora:
            print("è¿›è¡Œä½ç²¾åº¦è®­ç»ƒ")
            model = self._prepare_model_for_kbit_training(model, args)
            # Disable gradient checkpointing as it's handled by prepare_model_for_kbit_training
            args = dataclasses.replace(args, gradient_checkpointing=False)
        elif args.gradient_checkpointing:
            print("è¿›è¡Œé«˜ç²¾åº¦è®­ç»ƒ")
            model = self._enable_gradient_checkpointing(model, args)

        # Create PEFT model
        if (
            version.parse(speft.__version__) >= version.parse("0.12")  # autocast_adapter_dtype introduced in 0.12
            and getattr(model, "is_loaded_in_4bit", False)
            and is_sharded_qlora
        ):
            model = get_peft_model(model, peft_config, autocast_adapter_dtype=False)
            print("ç”Ÿæˆä½ç²¾åº¦å¾®è°ƒæ¨¡å‹")
        else:
            model = get_peft_model(model, peft_config)
            print("ç”Ÿæˆé«˜ç²¾åº¦å¾®è°ƒæ¨¡å‹")
            print(f"æ–°çš„æ¨¡å‹æ¶æ„ï¼š{model}")

        # Handle bf16 casting for 4-bit models
        if args.bf16 and getattr(model, "is_loaded_in_4bit", False) and not is_sharded_qlora:
            peft_module_casting_to_bf16(model)

        return model

    def _prepare_model_for_kbit_training(self, model: PreTrainedModel, args: SFTConfig) -> PreTrainedModel:
        """Prepares a quantized model for kbit training."""
        prepare_model_kwargs = {
            "use_gradient_checkpointing": args.gradient_checkpointing,
            "gradient_checkpointing_kwargs": args.gradient_checkpointing_kwargs or {},
        }

        return prepare_model_for_kbit_training(model, **prepare_model_kwargs)

    def _enable_gradient_checkpointing(self, model: PreTrainedModel, args: SFTConfig) -> PreTrainedModel:
        """Enables gradient checkpointing for the model."""
    # def ok123():
        print("ğŸ”§ å¼€å§‹é…ç½®æ¢¯åº¦æ£€æŸ¥ç‚¹ä¸æ¢¯åº¦éœ€æ±‚è®¾ç½®")
        
        # è·å–æ¢¯åº¦æ£€æŸ¥ç‚¹å‚æ•°é…ç½®
        gradient_checkpointing_kwargs = args.gradient_checkpointing_kwargs or {}
        print(f"ğŸ“¦ æ¢¯åº¦æ£€æŸ¥ç‚¹å‚æ•°: {gradient_checkpointing_kwargs} (é»˜è®¤ä½¿ç”¨ç©ºå­—å…¸)")

        # ç¡®å®šæ£€æŸ¥ç‚¹æ¨¡å¼
        use_reentrant = (
            "use_reentrant" not in gradient_checkpointing_kwargs 
            or gradient_checkpointing_kwargs["use_reentrant"]
        )
        print(f"ğŸ” æ£€æŸ¥use_reentrantæ ‡å¿—: {'æœªæŒ‡å®š' if 'use_reentrant' not in gradient_checkpointing_kwargs else 'æ˜¾å¼è®¾ç½®'}")
        print(f"ğŸ”„ ä½¿ç”¨{'å¯é‡å…¥(reentrant)' if use_reentrant else 'ä¸å¯é‡å…¥(non-reentrant)'}æ£€æŸ¥ç‚¹æ¨¡å¼")

        if use_reentrant:
            print("\nâš ï¸ æ£€æµ‹åˆ°éœ€è¦å¯ç”¨è¾“å…¥æ¢¯åº¦ä¿ç•™(reentrantæ¨¡å¼)")
            
            if hasattr(model, "enable_input_require_grads"):
                print("âœ… æ£€æµ‹åˆ°æ¨¡å‹å†…ç½®enable_input_require_gradsæ–¹æ³•")
                print("âš¡ æ­£åœ¨å¯ç”¨è‡ªåŠ¨è¾“å…¥æ¢¯åº¦éœ€æ±‚...")
                model.enable_input_require_grads()
                print("ğŸŸ¢ è‡ªåŠ¨æ¢¯åº¦éœ€æ±‚é…ç½®å®Œæˆ")
            else:
                print("â›” æ¨¡å‹æœªå®ç°enable_input_require_gradsæ–¹æ³•ï¼Œå¯ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
                
                def make_inputs_require_grad(module, input, output):
                    print(f"ğŸ¯ æ‰‹åŠ¨è®¾ç½®æ¢¯åº¦éœ€æ±‚ - æ¨¡å—: {module._get_name()}")
                    output.requires_grad_(True)
                    
                print("ğŸª æ³¨å†Œè¾“å…¥åµŒå…¥å±‚å‰å‘é’©å­...")
                embed_layer = model.get_input_embeddings()
                print(f"ğŸ”— ç›®æ ‡åµŒå…¥å±‚: {embed_layer.__class__.__name__}[in={embed_layer.num_embeddings}, dim={embed_layer.embedding_dim}]")
                hook_handle = embed_layer.register_forward_hook(make_inputs_require_grad)
                print(f"ğŸ“Œ é’©å­æ³¨å†ŒæˆåŠŸ (å¥æŸ„ID: {id(hook_handle)})")

        print("\nğŸ”š å®Œæˆæ¢¯åº¦æ£€æŸ¥ç‚¹é…ç½®")
        print("="*50)
        return model



    def _prepare_dataset(
        self,
        dataset: Union[Dataset, IterableDataset],
        processing_class: Union[PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin],
        args: SFTConfig,
        packing: bool,
        formatting_func: Optional[Callable[[dict], str]],
        dataset_name: str,
    ) -> Union[Dataset, IterableDataset]:
        print(f"\nğŸ” å¼€å§‹å¤„ç†æ•°æ®é›†: {dataset_name} (ç±»å‹: {type(dataset).__name__})")
        
        # 1. å¤„ç† ConstantLengthDataset ç±»å‹
        if isinstance(dataset, ConstantLengthDataset):
            print("ğŸ”„ æ£€æµ‹åˆ° ConstantLengthDatasetï¼Œè·³è¿‡é¢„å¤„ç†æ­¥éª¤")
            return dataset

        # 2. æ£€æŸ¥æ˜¯å¦å·²é¢„å¤„ç†
        first_sample = next(iter(dataset))
        column_names = list(first_sample.keys())
        is_processed = "input_ids" in column_names
        print(f"ğŸ“Š æ•°æ®é›†åˆ—å: {column_names}")
        print(f"ğŸ”„ é¢„å¤„ç†çŠ¶æ€: {'å·²å¤„ç†' if is_processed else 'æœªå¤„ç†'}")

        map_kwargs = {}
        if isinstance(dataset, Dataset):
            map_kwargs["num_proc"] = args.dataset_num_proc
            print(f"âš™ï¸ è®¾ç½®å¤šè¿›ç¨‹æ•°: {args.dataset_num_proc}")

        with PartialState().local_main_process_first():
            # 3. åº”ç”¨æ ¼å¼åŒ–å‡½æ•°
            if formatting_func is not None:
                if is_processed:
                    warnings.warn("å¿½ç•¥å·²å¤„ç†æ•°æ®é›†çš„æ ¼å¼åŒ–å‡½æ•°", UserWarning)
                    print("âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°å·²å¤„ç†æ•°æ®é›†ï¼Œè·³è¿‡ formatting_func")
                else:
                    print(f"ğŸ› ï¸ æ­£åœ¨åº”ç”¨æ ¼å¼åŒ–å‡½æ•°: {formatting_func.__name__}")
                    batched = isinstance(formatting_func(first_sample), list)
                    print(f"ğŸ“¦ æ‰¹å¤„ç†æ¨¡å¼: {'å¼€å¯' if batched else 'å…³é—­'}")

                    def _func(example):
                        result = {"text": formatting_func(example)}
                        print(f"ğŸ“ æ ¼å¼åŒ–æ ·ä¾‹ -> è¾“å…¥keys: {example.keys()} | è¾“å‡ºkeys: {result.keys()}")
                        return result

                    if isinstance(dataset, Dataset):
                        map_kwargs["desc"] = f"æ ¼å¼åŒ– {dataset_name}"
                    dataset = dataset.map(_func, batched=batched, **map_kwargs)
                    print(f"âœ… æ ¼å¼åŒ–å®Œæˆï¼Œæ–°åˆ—: {dataset.column_names}")

            # 4. åˆå¹¶ prompt/completion å­—æ®µ
            if "prompt" in column_names and "completion" in column_names:
                print("ğŸ”€ æ£€æµ‹åˆ° prompt-completion ç»“æ„ï¼Œå¼€å§‹åˆå¹¶...")
                key = "messages" if is_conversational(first_sample) else "text"
                print(f"ğŸ—ï¸ åˆå¹¶åçš„å­—æ®µå: {key}")

                def concat_prompt_completion(example):
                    merged = {key: example["prompt"] + example["completion"]}
                    print(f"âœ‚ï¸ åˆå¹¶æ ·ä¾‹ -> é•¿åº¦: {len(merged[key])} å­—ç¬¦")
                    return merged

                dataset = dataset.map(concat_prompt_completion, remove_columns=["prompt", "completion"])
                print(f"âœ… åˆå¹¶å®Œæˆï¼Œå‰©ä½™åˆ—: {dataset.column_names}")

        global bPrintMoreKv  # ç”¨äºæ§åˆ¶è¯¦ç»†è°ƒè¯•è¾“å‡ºçš„å…¨å±€å˜é‡
        
        # --- è½¬æ¢åˆ°ChatMLæ ¼å¼ ---
        print(f"\n=== é˜¶æ®µ1ï¼šå¼€å§‹è½¬æ¢æ•°æ®é›†åˆ°ChatMLæ ¼å¼ ===")
        if "conversations" in dataset.column_names:
            print(f"æ£€æµ‹åˆ°åŸå§‹å¯¹è¯åˆ—[conversations]ï¼Œå°†æ‰§è¡Œæ ¼å¼è½¬æ¢")
        else:
            print(f"æœªæ£€æµ‹åˆ°åŸå§‹å¯¹è¯åˆ—ï¼Œè·³è¿‡è½¬æ¢")

        if isinstance(dataset, Dataset):  # æ ‡å‡†æ•°æ®é›†æ”¯æŒè¿›åº¦æè¿°
            print(f"æ­£åœ¨ä½¿ç”¨æ ‡å‡†Datasetç±»å‹ï¼Œå¯ç”¨è¿›åº¦æ¡æ˜¾ç¤º")
            map_kwargs["desc"] = f"Converting {dataset_name} dataset to ChatML"
        else:
            print(f"ä½¿ç”¨IterableDatasetç±»å‹ï¼Œæ— è¿›åº¦æ¡æ˜¾ç¤º")

        dataset = dataset.map(
            maybe_convert_to_chatml,
            remove_columns="conversations" if "conversations" in dataset.column_names else None,
            **map_kwargs,
        )
        print(f"è½¬æ¢å®Œæˆï¼Œå½“å‰æ•°æ®é›†åˆ—åï¼š{dataset.column_names}")

        # --- åº”ç”¨èŠå¤©æ¨¡æ¿ ---
        print(f"\n=== é˜¶æ®µ2ï¼šåº”ç”¨èŠå¤©æ¨¡æ¿ ===")
        if "messages" in dataset.column_names:
            print(f"æ£€æµ‹åˆ°æ¶ˆæ¯åˆ—[messages]ï¼Œå°†åº”ç”¨æ¨¡æ¿")
        else:
            print(f"æœªæ£€æµ‹åˆ°æ¶ˆæ¯åˆ—ï¼Œè·³è¿‡æ¨¡æ¿åº”ç”¨")

        if isinstance(dataset, Dataset):
            map_kwargs["desc"] = f"Applying chat template to {dataset_name} dataset"
        dataset = dataset.map(
            maybe_apply_chat_template,
            fn_kwargs={"tokenizer": processing_class},
            remove_columns="messages" if "messages" in dataset.column_names else None,
            **map_kwargs,
        )
        print(f"æ¨¡æ¿åº”ç”¨å®Œæˆï¼Œå½“å‰æ•°æ®é›†åˆ—åï¼š{dataset.column_names}")
        if "text" in dataset.column_names:
            print(f"ç¤ºä¾‹æ–‡æœ¬å†…å®¹ï¼š\n{dataset[0]['text'][:100]}...")  # æ‰“å°é¦–æ¡æ–‡æœ¬å‰100å­—ç¬¦

        # --- åˆ†è¯å¤„ç† ---
        print(f"\n=== é˜¶æ®µ3ï¼šåˆ†è¯å¤„ç† ===")
        if not is_processed:
            print(f"æ•°æ®æœªé¢„å¤„ç†ï¼Œå¼€å§‹åˆ†è¯ï¼ˆå¤„ç†ç±»ï¼š{type(processing_class).__name__}ï¼‰")
            if isinstance(dataset, Dataset):
                map_kwargs["desc"] = f"Tokenizing {dataset_name} dataset"
            
            def tokenize(example, processing_class, dataset_text_field):
                print(f"æ­£åœ¨å¤„ç†æ ·æœ¬IDï¼š{example.get('id', 'N/A')}", " åˆ†è¯å¤„ç†çš„å­—æ®µåç§°ï¼š", dataset_text_field) if bPrintMoreKv else None
                # print("ä½¿ç”¨çš„å‡½æ•°ï¼š", processing_class) 
                result = processing_class(example[dataset_text_field])
                if bPrintMoreKv:
                    print(f"åˆ†è¯ç»“æœé•¿åº¦ï¼š{len(result['input_ids'])}")
                    print(f"ç¤ºä¾‹è¾“å…¥IDsï¼š{result['input_ids'][:10]}...")
                return result
                
            dataset = dataset.map(
                tokenize,
                fn_kwargs={"processing_class": processing_class, "dataset_text_field": args.dataset_text_field},
                **map_kwargs,
            )
            print(f"åˆ†è¯å®Œæˆï¼Œå½“å‰æ•°æ®é›†åˆ—åï¼š{dataset.column_names}")
        else:
            print("æ•°æ®å·²é¢„å¤„ç†ï¼Œè·³è¿‡åˆ†è¯æ­¥éª¤")

        # --- æ‰“åŒ…/æˆªæ–­å¤„ç† ---
        print(f"\n=== é˜¶æ®µ4ï¼šåºåˆ—æ•´ç† ===")
        if packing:
            print(f"å¯ç”¨æ‰“åŒ…æ¨¡å¼ï¼Œæœ€å¤§åºåˆ—é•¿åº¦ï¼š{args.max_seq_length}")
            if args.max_seq_length is None:
                raise ValueError("æ‰“åŒ…æ¨¡å¼å¿…é¡»æŒ‡å®šmax_seq_lengthå‚æ•°ï¼")
            
            print("ç­›é€‰ä¿ç•™input_idsåˆ—...")
            dataset = dataset.select_columns("input_ids")
            
            def pack_examples(examples, seq_length):
                print(f"æ‰¹é‡å¤„ç†æ ·æœ¬æ•°ï¼š{len(examples['input_ids'])}") if bPrintMoreKv else None
                # ...ï¼ˆå®é™…æ‰“åŒ…é€»è¾‘ï¼‰
                return packed_examples
                
            dataset = dataset.map(
                pack_examples, 
                batched=True, 
                fn_kwargs={"seq_length": args.max_seq_length}, 
                **map_kwargs
            )
            print(f"æ‰“åŒ…åæ•°æ®é›†ç»“æ„ï¼š{dataset}")
        elif args.max_seq_length is not None:
            print(f"å¯ç”¨æˆªæ–­æ¨¡å¼ï¼Œæœ€å¤§åºåˆ—é•¿åº¦ï¼š{args.max_seq_length}")
            
            def truncate(example, max_seq_length):
                if bPrintMoreKv:
                    print(f"æˆªæ–­å‰é•¿åº¦ï¼šinput_ids={len(example['input_ids'])}, attention_mask={len(example['attention_mask'])}")
                truncated = {key: val[:max_seq_length] for key, val in example.items() if key in ["input_ids", "attention_mask"]}
                if bPrintMoreKv:
                    print(f"æˆªæ–­åé•¿åº¦ï¼šinput_ids={len(truncated['input_ids'])}, attention_mask={len(truncated['attention_mask'])}")
                return truncated
                
            dataset = dataset.map(
                truncate,
                fn_kwargs={"max_seq_length": args.max_seq_length},
                **map_kwargs,
            )
            print(f"æˆªæ–­åç¤ºä¾‹é•¿åº¦ï¼š{len(dataset[0]['input_ids'])}")
        # else:
        #     print("æœªæŒ‡å®šmax_seq_lengthï¼Œè·³è¿‡åºåˆ—æ•´ç†")


            # For Liger kernel, ensure only input_ids is present
            if args.use_liger:
                print("è¡¥å……ids")
                dataset = dataset.select_columns("input_ids")

        # åŒ…å«ç½‘ç«™ä¸‹è½½çš„å­—æ®µï¼Œå’Œæ–°å¢çš„å­—æ®µ
        print("å¤„ç†å®Œè¿”å›çš„æ•°æ®ï¼š", dataset)
        return dataset

    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        """
        Compute training loss and additionally compute token accuracies
        """
        (loss, outputs) = super().compute_loss(
            model, inputs, return_outputs=True, num_items_in_batch=num_items_in_batch
        )

        # Compute token accuracy if we have labels and if the model is not using Liger (no logits)
        if "labels" in inputs and not self.args.use_liger:
            shift_logits = outputs.logits[..., :-1, :].contiguous()
            shift_labels = inputs["labels"][..., 1:].contiguous()

            # Get predictions
            predictions = shift_logits.argmax(dim=-1)

            # Create mask for non-padding tokens (assuming ignore_index is -100)
            mask = shift_labels != -100

            # Calculate accuracy only on non-padding tokens
            correct_predictions = (predictions == shift_labels) & mask
            total_tokens = mask.sum()
            correct_tokens = correct_predictions.sum()

            # Gather the correct_tokens and total_tokens across all processes
            correct_tokens = self.accelerator.gather_for_metrics(correct_tokens)
            total_tokens = self.accelerator.gather_for_metrics(total_tokens)

            # Compute the mean token accuracy and log it
            accuracy = (correct_tokens.sum() / total_tokens.sum()).item() if total_tokens.sum() > 0 else 0.0
            self._metrics["mean_token_accuracy"].append(accuracy)

        return (loss, outputs) if return_outputs else loss

    def log(self, logs: dict[str, float], start_time: Optional[float] = None) -> None:
        metrics = {key: sum(val) / len(val) for key, val in self._metrics.items()}  # average the metrics

        # This method can be called both in training and evaluation. When called in evaluation, the keys in `logs`
        # start with "eval_". We need to add the prefix "eval_" to the keys in `metrics` to match the format.
        if next(iter(logs.keys())).startswith("eval_"):
            metrics = {f"eval_{key}": val for key, val in metrics.items()}

        logs = {**logs, **metrics}
        if version.parse(transformers.__version__) >= version.parse("4.47.0.dev0"):
            super().log(logs, start_time)
        else:  # transformers<=4.46
            super().log(logs)
        self._metrics.clear()

    def create_model_card(
        self,
        model_name: Optional[str] = None,
        dataset_name: Optional[str] = None,
        tags: Union[str, list[str], None] = None,
    ):
        """
        Creates a draft of a model card using the information available to the `Trainer`.

        Args:
            model_name (`str` or `None`, *optional*, defaults to `None`):
                Name of the model.
            dataset_name (`str` or `None`, *optional*, defaults to `None`):
                Name of the dataset used for training.
            tags (`str`, `list[str]` or `None`, *optional*, defaults to `None`):
                Tags to be associated with the model card.
        """
        if not self.is_world_process_zero():
            return

        if hasattr(self.model.config, "_name_or_path") and not os.path.isdir(self.model.config._name_or_path):
            base_model = self.model.config._name_or_path
        else:
            base_model = None

        tags = tags or []
        if isinstance(tags, str):
            tags = [tags]

        if hasattr(self.model.config, "unsloth_version"):
            tags.append("unsloth")

        model_card = generate_model_card(
            base_model=base_model,
            model_name=model_name,
            hub_model_id=self.hub_model_id,
            dataset_name=dataset_name,
            tags=tags,
            wandb_url=wandb.run.get_url() if is_wandb_available() and wandb.run is not None else None,
            comet_url=get_comet_experiment_url(),
            trainer_name="SFT",
        )

        model_card.save(os.path.join(self.args.output_dir, "README.md"))
