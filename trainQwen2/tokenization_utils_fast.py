# coding=utf-8
# Copyright 2020 The HuggingFace Inc. team.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Tokenization classes for fast tokenizers (provided by HuggingFace's tokenizers library). For slow (python) tokenizers
see tokenization_utils.py
"""

import copy
import json
import os
from collections import defaultdict
from typing import Any, Dict, Iterable, List, Optional, Tuple, Union

import tokenizers.pre_tokenizers as pre_tokenizers_fast
from tokenizers import Encoding as EncodingFast
from tokenizers import Tokenizer as TokenizerFast
from tokenizers.decoders import Decoder as DecoderFast
from tokenizers.trainers import BpeTrainer, UnigramTrainer, WordLevelTrainer, WordPieceTrainer

from transformers.convert_slow_tokenizer import convert_slow_tokenizer
from transformers.integrations.ggml import convert_gguf_tokenizer
from transformers.modeling_gguf_pytorch_utils import load_gguf_checkpoint
from tokenization_utils import PreTrainedTokenizer
from tokenization_utils_base import (
    INIT_TOKENIZER_DOCSTRING,
    AddedToken,
    BatchEncoding,
    PreTokenizedInput,
    PreTokenizedInputPair,
    PreTrainedTokenizerBase,
    SpecialTokensMixin,
    TextInput,
    TextInputPair,
    TruncationStrategy,
)
from transformers.utils import PaddingStrategy, add_end_docstrings, logging


logger = logging.get_logger(__name__)

# Fast tokenizers (provided by HuggingFace tokenizer's library) can be saved in a single file
TOKENIZER_FILE = "tokenizer.json"
SPECIAL_TOKENS_MAP_FILE = "special_tokens_map.json"
TOKENIZER_CONFIG_FILE = "tokenizer_config.json"
TIKTOKEN_VOCAB_FILE = "tokenizer.model"

# Slow tokenizers have an additional added tokens files
ADDED_TOKENS_FILE = "added_tokens.json"

INIT_TOKENIZER_DOCSTRING += """
        tokenizer_object ([`tokenizers.Tokenizer`]):
            A [`tokenizers.Tokenizer`] object from ğŸ¤— tokenizers to instantiate from. See [Using tokenizers from ğŸ¤—
            tokenizers](../fast_tokenizers) for more information.
        tokenizer_file ([`str`]):
            A path to a local JSON file representing a previously serialized [`tokenizers.Tokenizer`] object from ğŸ¤—
            tokenizers.
"""

MODEL_TO_TRAINER_MAPPING = {
    "BPE": BpeTrainer,
    "Unigram": UnigramTrainer,
    "WordLevel": WordLevelTrainer,
    "WordPiece": WordPieceTrainer,
}

VOCAB_FILES_NAMES = {"tokenizer_file": TOKENIZER_FILE, "vocab_file": TIKTOKEN_VOCAB_FILE}


@add_end_docstrings(INIT_TOKENIZER_DOCSTRING)
class PreTrainedTokenizerFast(PreTrainedTokenizerBase):
    """
    Base class for all fast tokenizers (wrapping HuggingFace tokenizers library).

    Inherits from [`~tokenization_utils_base.PreTrainedTokenizerBase`].

    Handles all the shared methods for tokenization and special tokens, as well as methods for
    downloading/caching/loading pretrained tokenizers, as well as adding tokens to the vocabulary.

    This class also contains the added tokens in a unified way on top of all tokenizers so we don't have to handle the
    specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece...).
    """

    vocab_files_names = VOCAB_FILES_NAMES
    slow_tokenizer_class: PreTrainedTokenizer = None

    def __init__(self, *args, **kwargs):
        print("\n===== å¼€å§‹åˆå§‹åŒ–Faståˆ†è¯å™¨ =====")
        print("[DEBUG] è¾“å…¥å‚æ•°æ¦‚è§ˆ:")
        print(f"  argsé•¿åº¦: {len(args)}, kwargså…³é”®å­—: {list(kwargs.keys())}")

        # æå–å…³é”®å‚æ•°å¹¶æ‰“å°
        tokenizer_object = kwargs.pop("tokenizer_object", None)
        print(f"[PARAM] tokenizer_object: {'å­˜åœ¨' if tokenizer_object else 'None'}")

        slow_tokenizer = kwargs.pop("__slow_tokenizer", None)
        print(f"[PARAM] __slow_tokenizer: {'å­˜åœ¨' if slow_tokenizer else 'None'}")

        gguf_file = kwargs.pop("gguf_file", None)
        print(f"[PARAM] gguf_file: {gguf_file or 'æœªæä¾›'}")

        fast_tokenizer_file = kwargs.pop("tokenizer_file", None)
        print(f"[PARAM] tokenizer_file: {fast_tokenizer_file or 'æœªæä¾›'}")

        from_slow = kwargs.pop("from_slow", False)
        print(f"[PARAM] from_slow: {from_slow}")

        print("Q:ä¸ºä»€ä¹ˆè¦åŠ è¿™äº›å­—æ®µï¼Ÿå­—æ®µä»ä»€ä¹ˆä½ç½®è¿‡æ¥ï¼Ÿ")
        print("A:åŠ ä¸€äº›ç‰¹å®šè¯æ±‡ï¼Œå­—æ®µç»è¿‡tokençš„from_pretrainedå‡½æ•°è¯»å–tokenizer_config.jsoné…ç½®æ–‡ä»¶è·å–åˆ°")
        import sys, os
        print(f"\nQAè·³è½¬ File \"{os.path.abspath(__file__)}\", line {sys._getframe().f_lineno}")
        added_tokens_decoder = kwargs.pop("added_tokens_decoder", {})
        print(f"[PARAM] added_tokens_decoderæ¡ç›®æ•°: {len(added_tokens_decoder)}","å…·ä½“å†…å®¹ï¼š", added_tokens_decoder)

        # å¤„ç†å‰ç¼€ç©ºæ ¼å‚æ•°
        self.add_prefix_space = kwargs.get("add_prefix_space", False)
        print(f"[CONFIG] add_prefix_space: {self.add_prefix_space}")

        # æ£€æŸ¥æ…¢é€Ÿåˆ†è¯å™¨å…¼å®¹æ€§
        if from_slow and slow_tokenizer is None and self.slow_tokenizer_class is None:
            print("[ERROR] æ— æ³•ä»æ…¢é€Ÿåˆ†è¯å™¨å®ä¾‹åŒ–ï¼šç¼ºå°‘sentencepieceä¾èµ–ï¼")
            raise ValueError("Cannot instantiate...")

        # åˆå§‹åŒ–è·¯å¾„åˆ¤æ–­
        if tokenizer_object is not None:
            print("\n[BRANCH 1] ä»ç°æœ‰åˆ†è¯å™¨å¯¹è±¡æ·±åº¦å¤åˆ¶")
            print(f"  åŸå§‹å¯¹è±¡ç±»å‹: {type(tokenizer_object).__name__}")
            fast_tokenizer = copy.deepcopy(tokenizer_object)
            print(f"  å¤åˆ¶åå¯¹è±¡ID: {id(fast_tokenizer)} (åŸå§‹ID: {id(tokenizer_object)})")

        elif fast_tokenizer_file is not None and not from_slow:
            print(f"\n[BRANCH 2] ä»æ–‡ä»¶åŠ è½½: {fast_tokenizer_file}")
            try:
                print("  å°è¯•åŠ è½½tokenizersåº“åºåˆ—åŒ–æ–‡ä»¶...")
                print("Q:ä»ç³»ç»Ÿtokenizerså‡½æ•°å·²ç»å¯ä»¥è·å–åˆ°é…ç½®æ–‡ä»¶çš„add_specail_tokenså­—æ®µï¼Œä¸ºä»€ä¹ˆè¿˜å¾—åœ¨tokençš„from_pretrainedå‡½æ•°æå‰è·å–add_special_tokenså†…å®¹ï¼Ÿ")
                print("A:å› ä¸ºè¿™é‡Œçš„tokenizersæ˜¯ä»tokenizer.jsonè¯»å–æ•°æ®ï¼Œè€Œä¼ è¿›æ¥çš„add_special_tokensæ•°æ®æ¥è‡ªtokenizr_config.jsonï¼Œæ¥æºä¸ä¸€æ ·ï¼Œæœ€åå¾—æŠŠä¸¤è€…åˆå¹¶")
                import sys, os
                print(f"\nQAè·³è½¬ File \"{os.path.abspath(__file__)}\", line {sys._getframe().f_lineno}")
                fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file)
                print(f"  åŠ è½½æˆåŠŸï¼åˆ†è¯å™¨ç±»å‹: {type(fast_tokenizer).__name__}")
                print(f"  åˆå§‹è¯æ±‡é‡: {fast_tokenizer.get_vocab_size()}", "åˆ†è§£å™¨å†…å®¹ï¼š", fast_tokenizer)
            except Exception as e:
                print(f"[ERROR] æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")
                raise
        elif slow_tokenizer:
            print(f"\n[BRANCH 3] è½¬æ¢æ…¢é€Ÿåˆ†è¯å™¨: {type(slow_tokenizer).__name__}")
            print("  åŸå§‹æ…¢é€Ÿåˆ†è¯å™¨é…ç½®:")
            print(f"    vocabå¤§å°: {len(slow_tokenizer.vocab)}")
            print(f"    ç‰¹æ®ŠToken: {slow_tokenizer.all_special_tokens}")
            try:
                fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
                print("  è½¬æ¢æˆåŠŸï¼ç”Ÿæˆçš„åˆ†è¯å™¨:")
                print(f"    ç±»å‹: {type(fast_tokenizer).__name__}")
                print(f"    æ–°è¯æ±‡é‡: {fast_tokenizer.get_vocab_size()}")
            except Exception as e:
                print(f"[ERROR] è½¬æ¢å¤±è´¥: {str(e)}")
                raise

        elif gguf_file is not None:
            print(f"\n[BRANCH 4] å¤„ç†GGUFæ¨¡å‹æ–‡ä»¶: {gguf_file}")
            try:
                print("  åŠ è½½GGUFæ£€æŸ¥ç‚¹...")
                gguf_param = load_gguf_checkpoint(kwargs.get("vocab_file"))
                print(f"  æ¨¡å‹æ¶æ„: {gguf_param['config']['model_type']}")
                print(f"  åˆ†è¯å™¨é…ç½®é”®: {list(gguf_param['tokenizer'].keys())}")
                
                architecture = gguf_param["config"]["model_type"]
                tokenizer_dict = gguf_param["tokenizer"]
                tokenizer_config = gguf_param["tokenizer_config"]
                
                print("  å¼€å§‹è½¬æ¢GGUFåˆ†è¯å™¨...")
                fast_tokenizer, additional_kwargs = convert_gguf_tokenizer(architecture, tokenizer_dict)
                print(f"  è·å¾—é¢å¤–å‚æ•°: {list(additional_kwargs.keys())}")
                
                kwargs.update(tokenizer_config)
                if len(additional_kwargs) > 0:
                    kwargs.update(additional_kwargs)
                    print("  åˆå¹¶æ›´æ–°å‚æ•°åˆ°kwargs")
            except Exception as e:
                print(f"[ERROR] GGUFå¤„ç†å¤±è´¥: {str(e)}")
                raise

        elif self.slow_tokenizer_class is not None and slow_tokenizer is not False:
            print(f"\n[BRANCH 5] åŠ¨æ€åˆ›å»ºæ…¢é€Ÿåˆ†è¯å™¨: {self.slow_tokenizer_class.__name__}")
            print("  åˆå§‹åŒ–å‚æ•°:")
            print(f"    args: {args}")
            print(f"    kwargs: {kwargs}")
            
            try:
                slow_tokenizer = self.slow_tokenizer_class(*args, **kwargs)
                print("  æ…¢é€Ÿåˆ†è¯å™¨åˆ›å»ºæˆåŠŸï¼Œå¼€å§‹è½¬æ¢...")
                fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)
                print("  è½¬æ¢å®Œæˆ")
            except Exception as e:
                print(f"[ERROR] æ…¢é€Ÿåˆ†è¯å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                raise

        elif not slow_tokenizer:
            print("\n[BRANCH 6] ä½¿ç”¨TikTokenå…¼å®¹æ¨¡å¼")
            self.vocab_file = kwargs.get("vocab_file", None)
            print(f"  ä½¿ç”¨è¯æ±‡è¡¨æ–‡ä»¶: {self.vocab_file}")
            
            self.additional_special_tokens = kwargs.get("additional_special_tokens", [])
            print(f"  é¢å¤–ç‰¹æ®ŠToken: {self.additional_special_tokens}")
            
            try:
                print("  å°è¯•TikTokenè½¬æ¢...")
                fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)
                print("  TikTokenè½¬æ¢æˆåŠŸ")
            except Exception as e:
                print(f"[ERROR] TikTokenè½¬æ¢å¤±è´¥: {str(e)}")
                raise

        else:
            print("\n[ERROR] æ— æ³•åŒ¹é…ä»»ä½•åˆå§‹åŒ–è·¯å¾„ï¼")
            print("  å‰©ä½™å‚æ•°:")
            print(f"    args: {args}")
            print(f"    kwargs: {kwargs}")
            raise ValueError("Couldn't instantiate the backend tokenizer...")

        print("[åˆå§‹åŒ–] å¼€å§‹è®¾ç½®Fast Tokenizerå‚æ•°")
        self._tokenizer = fast_tokenizer
        print(f"[é…ç½®] å·²ç»‘å®šFast Tokenizerå¯¹è±¡: {type(self._tokenizer)}")

        # æ…¢é€ŸTokenizerå…¼å®¹é€»è¾‘
        if slow_tokenizer is not None:
            print("[å…¼å®¹] æ£€æµ‹åˆ°Slow Tokenizerï¼Œåˆå¹¶åˆå§‹åŒ–å‚æ•°")
            kwargs.update(slow_tokenizer.init_kwargs)
            print(f"[å‚æ•°] æ›´æ–°åkwargs: {list(kwargs.keys())}")

        self._decode_use_source_tokenizer = False
        print(f"[è§£ç ] è®¾ç½®è§£ç ä¸ä½¿ç”¨æºåˆ†è¯å™¨: {self._decode_use_source_tokenizer}")

        # æˆªæ–­é…ç½®
        _truncation = self._tokenizer.truncation
        if _truncation is not None:
            print("\n[æˆªæ–­] å¯ç”¨æˆªæ–­ç­–ç•¥ï¼Œå‚æ•°è¯¦æƒ…:")
            print(f"  - æœ€å¤§é•¿åº¦(max_length): {_truncation.get('max_length', 'æœªè®¾ç½®')}")
            print(f"  - æ–¹å‘(direction): {_truncation.get('direction', 'æœªè®¾ç½®')}")
            print(f"  - æ­¥é•¿(stride): {_truncation.get('stride', 'æœªè®¾ç½®')}")
            print(f"  - ç­–ç•¥(strategy): {_truncation.get('strategy', 'æœªè®¾ç½®')}")
            
            self._tokenizer.enable_truncation(**_truncation)
            kwargs.setdefault("max_length", _truncation["max_length"])
            kwargs.setdefault("truncation_side", _truncation["direction"])
            kwargs.setdefault("stride", _truncation["stride"])
            kwargs.setdefault("truncation_strategy", _truncation["strategy"])
            print("[æˆªæ–­] å‚æ•°å·²æ³¨å…¥kwargs:", {k: kwargs[k] for k in ["max_length", "truncation_side", "stride", "truncation_strategy"]})
        else:
            print("[æˆªæ–­] æœªæ£€æµ‹åˆ°æˆªæ–­é…ç½®ï¼Œç¦ç”¨æˆªæ–­")
            self._tokenizer.no_truncation()

        # å¡«å……é…ç½®
        _padding = self._tokenizer.padding
        if _padding is not None:
            print("\n[å¡«å……] å¯ç”¨å¡«å……ç­–ç•¥ï¼Œå‚æ•°è¯¦æƒ…:")
            print(f"  - å¡«å……ç¬¦(pad_token): {_padding.get('pad_token', 'æœªè®¾ç½®')}")
            print(f"  - æ–¹å‘(direction): {_padding.get('direction', 'æœªè®¾ç½®')}")
            print(f"  - æœ€å¤§é•¿åº¦(length): {_padding.get('length', 'æœªè®¾ç½®')}")
            print(f"  - å€æ•°å¯¹é½(pad_to_multiple_of): {_padding.get('pad_to_multiple_of', 'æœªè®¾ç½®')}")
            
            self._tokenizer.enable_padding(**_padding)
            kwargs.setdefault("pad_token", _padding["pad_token"])
            kwargs.setdefault("pad_token_type_id", _padding["pad_type_id"])
            kwargs.setdefault("padding_side", _padding["direction"])
            kwargs.setdefault("max_length", _padding["length"])
            kwargs.setdefault("pad_to_multiple_of", _padding["pad_to_multiple_of"])
            print("[å¡«å……] å‚æ•°å·²æ³¨å…¥kwargs:", {k: kwargs[k] for k in ["pad_token", "padding_side", "max_length", "pad_to_multiple_of"]})
        else:
            print("[å¡«å……] æœªæ£€æµ‹åˆ°å¡«å……é…ç½®ï¼Œç¦ç”¨å¡«å……")

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        print("\n[ç»§æ‰¿] æ‰§è¡Œçˆ¶ç±»åˆå§‹åŒ–ï¼Œæœ€ç»ˆå‚æ•°:")
        for k, v in kwargs.items():
            print(f"  - {k}: {v}" if len(str(v)) < 50 else f"  - {k}: ...ï¼ˆé•¿åº¦{len(str(v))}ï¼‰")
        super().__init__(**kwargs)

        # ç‰¹æ®ŠTokenå¤„ç†
        print("\n[ç‰¹æ®Šæ ‡è®°] é…ç½®ç¼–ç è§£ç è¡Œä¸º")
        self._tokenizer.encode_special_tokens = self.split_special_tokens
        # print(f"Encodeç‰¹æ®Šæ ‡è®°æ–¹æ³•ç»‘å®š: {self.split_special_tokens.__name__}")

        # æ–°å¢Tokenå»é‡é€»è¾‘
        added_tokens_decoder_hash = {hash(repr(token)) for token in self.added_tokens_decoder}
        print(f"\n[å»é‡] ç°æœ‰å·²æ·»åŠ Tokenå“ˆå¸Œå€¼æ•°é‡: {len(added_tokens_decoder_hash)}")
        
        tokens_to_add = [
            token
            for index, token in sorted(added_tokens_decoder.items(), key=lambda x: x[0])
            if hash(repr(token)) not in added_tokens_decoder_hash
        ]
        print(f"[æ–°å¢] éœ€è¦æ·»åŠ çš„å”¯ä¸€Tokenæ•°é‡: {len(tokens_to_add)}")

        # åˆå¹¶ç‰¹æ®ŠToken
        print("Qï¼šæ–°å¢çš„åŠ å¯†tokenå’Œè§£å¯†tokenæ˜¯ä¸€æ ·çš„ï¼Ÿ")
        print("Aï¼šæ˜¯çš„ï¼Œä»ä»£ç ä¸Šå¯ä»¥çœ‹åˆ°æ˜¯ä¸€æ ·çš„")
        import sys, os
        print(f"\nQAè·³è½¬ File \"{os.path.abspath(__file__)}\", line {sys._getframe().f_lineno}")
        encoder = list(self.added_tokens_encoder.keys()) + [str(token) for token in tokens_to_add]
        print(f"[åˆå¹¶] å½“å‰ç¼–ç å™¨æ€»Tokenæ•°: {len(encoder)}")
        
        special_tokens = [
            token 
            for token in self.all_special_tokens_extended 
            if token not in encoder and token not in tokens_to_add
        ]
        tokens_to_add += special_tokens
        print(f"[ç‰¹æ®Š] è¿½åŠ é¢„å®šä¹‰ç‰¹æ®ŠTokenæ•°é‡: {len(special_tokens)}")

        if len(tokens_to_add) > 0:
            print(f"\n[æ“ä½œ] å¼€å§‹æ·»åŠ {len(tokens_to_add)}ä¸ªTokenåˆ°åˆ†è¯å™¨")
            for i, token in enumerate(tokens_to_add, 1):
                is_special = (
                    (token.special or str(token) in self.all_special_tokens)
                    if isinstance(token, AddedToken)
                    else str(token) in self.all_special_tokens
                )
                print(f"  Token {i}: {str(token)[:20]}... | æ˜¯å¦ç‰¹æ®Š: {is_special}")
            self.add_tokens(tokens_to_add)
        else:
            print("[æ— æ“ä½œ] æ²¡æœ‰éœ€è¦æ·»åŠ çš„æ–°Token")

        # é…ç½®æ–‡ä»¶åŠ è½½
        try:
            print("\n[é…ç½®] å°è¯•åŠ è½½tokenizer.jsonæ–‡ä»¶")
            tokenizer_config = json.load(open("tokenizer.json"))
            print(f"  - æ–‡ä»¶ç‰ˆæœ¬: {tokenizer_config.get('version', 'æœªçŸ¥')}")
            print(f"  - æ¨¡å‹ç±»å‹: {tokenizer_config.get('model_type', 'æœªçŸ¥')}")
        except Exception as e:
            print(f"[é”™è¯¯] é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")


    @property
    def is_fast(self) -> bool:
        return True

    @property
    def can_save_slow_tokenizer(self) -> bool:
        """
        `bool`: Whether or not the slow tokenizer can be saved. Usually for sentencepiece based slow tokenizer, this
        can only be `True` if the original `"sentencepiece.model"` was not deleted.
        """
        return True

    @property
    def vocab_size(self) -> int:
        """
        `int`: Size of the base vocabulary (without the added tokens).
        """
        return self._tokenizer.get_vocab_size(with_added_tokens=False)

    def get_vocab(self) -> Dict[str, int]:
        return self._tokenizer.get_vocab(with_added_tokens=True)

    @property
    def vocab(self) -> Dict[str, int]:
        return self.get_vocab()

    @property
    def added_tokens_encoder(self) -> Dict[str, int]:
        """
        Returns the sorted mapping from string to index. The added tokens encoder is cached for performance
        optimisation in `self._added_tokens_encoder` for the slow tokenizers.
        """
        return {k.content: v for v, k in sorted(self.added_tokens_decoder.items(), key=lambda item: item[0])}

    @property
    def added_tokens_decoder(self) -> Dict[int, AddedToken]:
        """
        Returns the added tokens in the vocabulary as a dictionary of index to AddedToken.

        Returns:
            `Dict[str, int]`: The added tokens.
        """
        return self._tokenizer.get_added_tokens_decoder()

    def get_added_vocab(self) -> Dict[str, int]:
        """
        Returns the added tokens in the vocabulary as a dictionary of token to index.

        Returns:
            `Dict[str, int]`: The added tokens.
        """
        return {k.content: v for v, k in sorted(self.added_tokens_decoder.items(), key=lambda item: item[0])}

    def __len__(self) -> int:
        """
        Size of the full vocabulary with the added tokens.
        """
        # print("åŠ å¯†ä¸­=========")
        return self._tokenizer.get_vocab_size(with_added_tokens=True)

    @property
    def backend_tokenizer(self) -> TokenizerFast:
        """
        `tokenizers.implementations.BaseTokenizer`: The Rust tokenizer used as a backend.
        """
        # print("åŠ å¯†ä¸­=========")
        return self._tokenizer

    @property
    def decoder(self) -> DecoderFast:
        """
        `tokenizers.decoders.Decoder`: The Rust decoder for this tokenizer.
        """
        return self._tokenizer.decoder

    def _convert_encoding(
        self,
        encoding: EncodingFast,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_offsets_mapping: bool = False,
        return_length: bool = False,
        verbose: bool = True,
    ) -> Tuple[Dict[str, Any], List[EncodingFast]]:
        """
        Convert the encoding representation to python Dict and list of encodings.
        æ·»åŠ è¯¦ç»†æ‰“å°ä¿¡æ¯è·Ÿè¸ªç¼–ç è½¬æ¢æµç¨‹
        """
        print("\n=== å¼€å§‹ç¼–ç è½¬æ¢ ===")
        print(f"ä¼ å…¥å‚æ•°ï¼šreturn_overflowing_tokens={return_overflowing_tokens}, return_offsets_mapping={return_offsets_mapping}")

        # å‚æ•°åˆå§‹åŒ–éªŒè¯
        return_token_type_ids = return_token_type_ids if return_token_type_ids is not None else "token_type_ids" in self.model_input_names
        return_attention_mask = return_attention_mask if return_attention_mask is not None else "attention_mask" in self.model_input_names
        print(f"è‡ªåŠ¨è®¾ç½®å‚æ•°ï¼štoken_type_ids={return_token_type_ids}, attention_mask={return_attention_mask}")

        # å¤„ç†æº¢å‡ºtoken
        overflow_status = "æ£€æµ‹åˆ°æº¢å‡º" if encoding.overflowing else "æ— æº¢å‡º"
        print(f"\næº¢å‡ºæ£€æµ‹ï¼š{overflow_status}")
        if return_overflowing_tokens and encoding.overflowing:
            encodings = [encoding] + encoding.overflowing
            print(f"å½“å‰æ€»ç¼–ç æ•°ï¼š{len(encodings)} (åŸå§‹+{len(encoding.overflowing)}æº¢å‡º)")
        else:
            encodings = [encoding]
            print("æœªå¯ç”¨æº¢å‡ºtokenè¿”å›æˆ–æ— å¯æº¢å‡ºç¼–ç ")

        # æ„å»ºç¼–ç å­—å…¸
        print("\næ„å»ºç¼–ç å­—å…¸ï¼š")
        encoding_dict = defaultdict(list)
        for idx, e in enumerate(encodings):
            print(f"\nå¤„ç†ç¬¬ {idx+1}/{len(encodings)} ä¸ªç¼–ç ï¼š")
            
            # æ ¸å¿ƒå­—æ®µ
            encoding_dict["input_ids"].append(e.ids)
            print(f"æ·»åŠ input_idsï¼ˆé•¿åº¦ï¼š{len(e.ids)}ï¼‰")
            
            # å¯é€‰å­—æ®µ
            if return_token_type_ids:
                encoding_dict["token_type_ids"].append(e.type_ids)
                print(f"æ·»åŠ token_type_idsï¼ˆé•¿åº¦ï¼š{len(e.type_ids)}ï¼‰")
            
            if return_attention_mask:
                encoding_dict["attention_mask"].append(e.attention_mask)
                print(f"æ·»åŠ attention_maskï¼ˆé•¿åº¦ï¼š{len(e.attention_mask)}ï¼‰")
            
            if return_special_tokens_mask:
                encoding_dict["special_tokens_mask"].append(e.special_tokens_mask)
                print(f"æ·»åŠ special_tokens_maskï¼ˆé•¿åº¦ï¼š{len(e.special_tokens_mask)}ï¼‰")
            
            if return_offsets_mapping:
                encoding_dict["offset_mapping"].append(e.offsets)
                print(f"æ·»åŠ offset_mappingï¼ˆé¦–é¡¹ï¼š{e.offsets[0] if e.offsets else None}ï¼‰")
            
            if return_length:
                encoding_dict["length"].append(len(e.ids))
                print(f"æ·»åŠ lengthå€¼ï¼š{len(e.ids)}")

        # æœ€ç»ˆè¾“å‡ºéªŒè¯
        print("\nè½¬æ¢å®Œæˆï¼Œè¾“å‡ºç»“æ„ï¼š")
        print(f"ç”Ÿæˆå­—æ®µåˆ—è¡¨ï¼š{list(encoding_dict.keys())}")
        print(f"æ€»ç¼–ç æ•°ï¼š{len(encodings)}")
        print(f"é¦–ç¼–ç input_idsé•¿åº¦ï¼š{len(encodings[0].ids) if encodings else 0}")
        
        return encoding_dict, encodings


    def convert_tokens_to_ids(self, tokens: Union[str, Iterable[str]]) -> Union[int, List[int]]:
        """
        Converts a token string (or a sequence of tokens) in a single integer id (or a Iterable of ids), using the
        vocabulary.

        Args:
            tokens (`str` or `Iterable[str]`): One or several token(s) to convert to token id(s).

        Returns:
            `int` or `List[int]`: The token id or list of token ids.
        """
        # print("åŠ å¯†ä¸­=========")
        if isinstance(tokens, str):
            return self._convert_token_to_id_with_added_voc(tokens)

        return [self._convert_token_to_id_with_added_voc(token) for token in tokens]

    def _convert_token_to_id_with_added_voc(self, token: str) -> int:
        # # print("åŠ å¯†ä¸­=========")
        index = self._tokenizer.token_to_id(token)
        if index is None:
            return self.unk_token_id
        return index

    def _convert_id_to_token(self, index: int) -> Optional[str]:
        # # print("åŠ å¯†ä¸­=========")
        return self._tokenizer.id_to_token(int(index))

    def _add_tokens(self, new_tokens: List[Union[str, AddedToken]], special_tokens=False) -> int:
        # # print("åŠ å¯†ä¸­=========")
        if special_tokens:
            return self._tokenizer.add_special_tokens(new_tokens)

        print("tokenç±»å‹ï¼š", self._tokenizer)
        return self._tokenizer.add_tokens(new_tokens)

    def num_special_tokens_to_add(self, pair: bool = False) -> int:
        """
        Returns the number of added tokens when encoding a sequence with special tokens.

        <Tip>

        This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not put
        this inside your training loop.

        </Tip>

        Args:
            pair (`bool`, *optional*, defaults to `False`):
                Whether the number of added tokens should be computed in the case of a sequence pair or a single
                sequence.

        Returns:
            `int`: Number of special tokens added to sequences.
        """
        return self._tokenizer.num_special_tokens_to_add(pair)

    def convert_ids_to_tokens(
        self, ids: Union[int, List[int]], skip_special_tokens: bool = False
    ) -> Union[str, List[str]]:
        """
        Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and
        added tokens.

        Args:
            ids (`int` or `List[int]`):
                The token id (or token ids) to convert to tokens.
            skip_special_tokens (`bool`, *optional*, defaults to `False`):
                Whether or not to remove special tokens in the decoding.

        Returns:
            `str` or `List[str]`: The decoded token(s).
        """
        print("è¿›è¡Œè½¬æ¢")
        if isinstance(ids, int):
            return self._tokenizer.id_to_token(ids)
        tokens = []
        for index in ids:
            index = int(index)
            if skip_special_tokens and index in self.all_special_ids:
                continue
            tokens.append(self._tokenizer.id_to_token(index))
        return tokens

    def tokenize(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> List[str]:
        print("ç”Ÿæˆå£ä»¤")
        return self.encode_plus(text=text, text_pair=pair, add_special_tokens=add_special_tokens, **kwargs).tokens()

    def set_truncation_and_padding(
        self,
        padding_strategy: PaddingStrategy,
        truncation_strategy: TruncationStrategy,
        max_length: int,
        stride: int,
        pad_to_multiple_of: Optional[int],
        padding_side: Optional[bool],
    ):
        """
        Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers
        library) and restore the tokenizer settings afterwards.

        The provided tokenizer has no padding / truncation strategy before the managed section. If your tokenizer set a
        padding / truncation strategy before, then it will be reset to no padding / truncation when exiting the managed
        section.

        Args:
            padding_strategy ([`~utils.PaddingStrategy`]):
                The kind of padding that will be applied to the input
            truncation_strategy ([`~tokenization_utils_base.TruncationStrategy`]):
                The kind of truncation that will be applied to the input
            max_length (`int`):
                The maximum size of a sequence.
            stride (`int`):
                The stride to use when handling overflow.
            pad_to_multiple_of (`int`, *optional*):
                If set will pad the sequence to a multiple of the provided value. This is especially useful to enable
                the use of Tensor Cores on NVIDIA hardware with compute capability `>= 7.5` (Volta).
            padding_side (`str`, *optional*):
                The side on which the model should have padding applied. Should be selected between ['right', 'left'].
                Default value is picked from the class attribute of the same name.
        """
        # print("åŠ å¯†ä¸­=========")
        _truncation = self._tokenizer.truncation
        _padding = self._tokenizer.padding
        # Set truncation and padding on the backend tokenizer
        if truncation_strategy == TruncationStrategy.DO_NOT_TRUNCATE:
            if _truncation is not None:
                self._tokenizer.no_truncation()
        else:
            target = {
                "max_length": max_length,
                "stride": stride,
                "strategy": truncation_strategy.value,
                "direction": self.truncation_side,
            }

            # _truncation might contain more keys that the target `transformers`
            # supports. Use only the target keys to trigger `enable_truncation`.
            # This should enable this code to works on various `tokenizers`
            # targets.
            if _truncation is None:
                current = None
            else:
                current = {k: _truncation.get(k, None) for k in target}

            if current != target:
                self._tokenizer.enable_truncation(**target)

        if padding_strategy == PaddingStrategy.DO_NOT_PAD:
            if _padding is not None:
                self._tokenizer.no_padding()
        else:
            length = max_length if padding_strategy == PaddingStrategy.MAX_LENGTH else None
            target = {
                "length": length,
                "direction": padding_side if padding_side is not None else self.padding_side,
                "pad_id": self.pad_token_id,
                "pad_token": self.pad_token,
                "pad_type_id": self.pad_token_type_id,
                "pad_to_multiple_of": pad_to_multiple_of,
            }
            if _padding != target:
                self._tokenizer.enable_padding(**target)

    def _batch_encode_plus(
        self,
        batch_text_or_text_pairs: Union[
            List[TextInput], List[TextInputPair], List[PreTokenizedInput], List[PreTokenizedInputPair]
        ],
        add_special_tokens: bool = True,
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
        max_length: Optional[int] = None,
        stride: int = 0,
        is_split_into_words: bool = False,
        pad_to_multiple_of: Optional[int] = None,
        padding_side: Optional[bool] = None,
        return_tensors: Optional[str] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_offsets_mapping: bool = False,
        return_length: bool = False,
        verbose: bool = True,
        split_special_tokens: bool = False,
    ) -> BatchEncoding:
        print("è¿›è¡Œç»†èŠ‚æ“ä½œ")
    # def ok():
        # è¾“å…¥ç±»å‹æ£€æŸ¥
        print(f"å¼€å§‹è¾“å…¥ç±»å‹æ£€æŸ¥ï¼Œè¾“å…¥ç±»å‹ä¸ºï¼š{type(batch_text_or_text_pairs)}")
        if not isinstance(batch_text_or_text_pairs, (tuple, list)):
            raise TypeError(
                f"batch_text_or_text_pairs has to be a list or a tuple (got {type(batch_text_or_text_pairs)})"
            )
        print("âˆš è¾“å…¥ç±»å‹æ£€æŸ¥é€šè¿‡ï¼Œç±»å‹ä¸ºåˆ—è¡¨/å…ƒç»„")

        # è®¾ç½®æˆªæ–­ä¸å¡«å……ç­–ç•¥
        print("\nå¼€å§‹è®¾ç½®æˆªæ–­ä¸å¡«å……ç­–ç•¥...")
        print(f"å‚æ•°è¯¦æƒ…: padding_strategy={padding_strategy}, truncation_strategy={truncation_strategy}, "
              f"max_length={max_length}, stride={stride}, pad_to_multiple_of={pad_to_multiple_of}")
        self.set_truncation_and_padding(
            padding_strategy=padding_strategy,
            truncation_strategy=truncation_strategy,
            max_length=max_length,
            stride=stride,
            pad_to_multiple_of=pad_to_multiple_of,
            padding_side=padding_side,
        )
        print("âˆš ç­–ç•¥è®¾ç½®å®Œæˆ")

        # æ£€æŸ¥ç‰¹æ®Štokenåˆ†å‰²è®¾ç½®
        print(f"\næ£€æŸ¥ç‰¹æ®Štokenåˆ†å‰²: å½“å‰split_special_tokens={split_special_tokens}ï¼Œ"
              f"ä¸å½“å‰è®¾ç½®{self._tokenizer.encode_special_tokens}æ˜¯å¦ä¸€è‡´ï¼Ÿ")
        if self._tokenizer.encode_special_tokens != split_special_tokens:
            self._tokenizer.encode_special_tokens = split_special_tokens
            print("â†’ æ£€æµ‹åˆ°ä¸ä¸€è‡´ï¼Œå·²æ›´æ–°split_special_tokensè®¾ç½®")
        else:
            print("â†’ è®¾ç½®ä¸€è‡´ï¼Œæ— éœ€ä¿®æ”¹")

        # æ‰¹é‡ç¼–ç è¿‡ç¨‹
        print("\nå¼€å§‹æ‰¹é‡ç¼–ç ...")
        print(f"å‚æ•°è¯¦æƒ…: add_special_tokens={add_special_tokens}, is_split_into_words={is_split_into_words}")
        print(f"æ ·æœ¬æ•°é‡: {len(batch_text_or_text_pairs)}")
        print("é¦–æ ·æœ¬ç¤ºä¾‹:", batch_text_or_text_pairs[0][:50] + "...", "æ ·æ¿é•¿åº¦ï¼š", len(batch_text_or_text_pairs[0]))  # æ‰“å°é¦–æ ·æœ¬å‰50å­—ç¬¦
        
        # print("ä½¿ç”¨çš„_tokenizerç±»ï¼š", self._tokenizer)
        encodings = self._tokenizer.encode_batch(
            batch_text_or_text_pairs,
            add_special_tokens=add_special_tokens,
            is_pretokenized=is_split_into_words,
        )
        print(f"âˆš ç¼–ç å®Œæˆï¼Œå…±è·å¾—{len(encodings)}ä¸ªç¼–ç ç»“æœ")
        print("é¦–ç¼–ç ç»“æ„ç¤ºä¾‹:", type(encodings[0]), "é•¿åº¦:", len(encodings[0]), "å‰é¢50ä¸ªå†…å®¹ï¼š", encodings[0])

        # ç¼–ç ç»“æœè½¬æ¢
        print("\nå¼€å§‹ç¼–ç è½¬æ¢...")
        print(f"è¿”å›å‚æ•°: return_token_type_ids={return_token_type_ids}, return_attention_mask={return_attention_mask}")
        tokens_and_encodings = [
            self._convert_encoding(
                encoding=encoding,
                return_token_type_ids=return_token_type_ids,
                return_attention_mask=return_attention_mask,
                return_overflowing_tokens=return_overflowing_tokens,
                return_special_tokens_mask=return_special_tokens_mask,
                return_offsets_mapping=return_offsets_mapping,
                return_length=return_length,
                verbose=verbose,
            )
            for encoding in encodings
        ]
        print("âˆš è½¬æ¢å®Œæˆ")
        print("é¦–å…ƒç´ è½¬æ¢ç»“æœç¤ºä¾‹ - å­—å…¸é”®:", tokens_and_encodings[0][0].keys())
        print("é¦–å…ƒç´ ç¼–ç ä¿¡æ¯ç±»å‹:", type(tokens_and_encodings[0][1]))

        # æ•°æ®æ ¼å¼æ•´ç†
        print("\nå¼€å§‹æ•°æ®æ¸…æ´—ä¸æ ¼å¼æ•´ç†...")
        sanitized_tokens = {}
        for key in tokens_and_encodings[0][0].keys():
            stack = [e for item, _ in tokens_and_encodings for e in item[key]]
            sanitized_tokens[key] = stack
            print(f"å­—æ®µ {key} æ•°æ®é‡: {len(stack)}")  # å„å­—æ®µæ•°æ®é‡ç»Ÿè®¡
        
        sanitized_encodings = [e for _, item in tokens_and_encodings for e in item]
        print(f"æ¸…æ´—åæ€»ç¼–ç æ•°: {len(sanitized_encodings)}")


        print("\n=== å¼€å§‹å¤„ç†æº¢å‡ºtokenæ˜ å°„ ===")
        
        # æº¢å‡ºtokenæ˜ å°„å¤„ç†
        overflow_to_sample_mapping = []
        if return_overflowing_tokens:
            print("\nç”Ÿæˆæº¢å‡ºtokenæ˜ å°„å…³ç³»...")
            print(f"æ€»æ ·æœ¬æ•°: {len(tokens_and_encodings)}")
            
            for i, (toks, _) in enumerate(tokens_and_encodings):
                print(f"\næ­£åœ¨å¤„ç†æ ·æœ¬ {i}:")
                print("åŸå§‹tokenæ•°:", len(toks['input_ids']))
                
                # è·å–å½“å‰æ ·æœ¬æº¢å‡ºæ¬¡æ•°
                overflow_count = len(toks['input_ids'])  # æ¯ä¸ªå—è§†ä¸ºä¸€æ¬¡"æº¢å‡º"
                print(f"æº¢å‡ºæ¬¡æ•°è®¡ç®—: len(input_ids) = {overflow_count}")
                
                # ç”Ÿæˆæ˜ å°„å…³ç³»
                mapping_segment = [i] * overflow_count
                print(f"æ·»åŠ æ˜ å°„æ®µ: {mapping_segment}")
                
                overflow_to_sample_mapping += mapping_segment
                print(f"æ›´æ–°åçš„æ˜ å°„æ•°ç»„: {overflow_to_sample_mapping[-overflow_count:]}")

                print(f"æ ·æœ¬ {i} å¤„ç†å®Œæˆï¼Œç´¯è®¡æ˜ å°„æ•°: {len(overflow_to_sample_mapping)}")
            
            print("\næœ€ç»ˆæ˜ å°„æ•°ç»„ç”Ÿæˆ:")
            print(f"æ€»æ˜ å°„æ•°: {len(overflow_to_sample_mapping)}")
            print("å‰10ä¸ªæ˜ å°„ç´¢å¼•:", overflow_to_sample_mapping[:10])
            
            sanitized_tokens["overflow_to_sample_mapping"] = overflow_to_sample_mapping
            print("\næ˜ å°„å…³ç³»å·²å­˜å…¥sanitized_tokens")
        
        # é•¿åº¦æ ¡éªŒ
        print("\nå¼€å§‹åºåˆ—é•¿åº¦æ ¡éªŒ...")
        for idx, input_ids in enumerate(sanitized_tokens["input_ids"]):
            print(f"æ ¡éªŒåºåˆ— {idx}ï¼Œé•¿åº¦: {len(input_ids)}")
            self._eventual_warn_about_too_long_sequence(input_ids, max_length, verbose)
        
        print("\nç”Ÿæˆæœ€ç»ˆBatchEncodingå¯¹è±¡")
        return BatchEncoding(
            sanitized_tokens, 
            sanitized_encodings, 
            tensor_type=return_tensors
        )


    def _encode_plus(
        self,
        text: Union[TextInput, PreTokenizedInput],
        text_pair: Optional[Union[TextInput, PreTokenizedInput]] = None,
        add_special_tokens: bool = True,
        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,
        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,
        max_length: Optional[int] = None,
        stride: int = 0,
        is_split_into_words: bool = False,
        pad_to_multiple_of: Optional[int] = None,
        padding_side: Optional[bool] = None,
        return_tensors: Optional[bool] = None,
        return_token_type_ids: Optional[bool] = None,
        return_attention_mask: Optional[bool] = None,
        return_overflowing_tokens: bool = False,
        return_special_tokens_mask: bool = False,
        return_offsets_mapping: bool = False,
        return_length: bool = False,
        verbose: bool = True,
        split_special_tokens: bool = False,
        **kwargs,
    ) -> BatchEncoding:
    # def ok():
        print("å¼€å§‹æ‰§è¡Œokæ–¹æ³•ï¼Œå‡†å¤‡å¤„ç†è¾“å…¥æ–‡æœ¬/æ–‡æœ¬å¯¹...")
        
        # æ„é€ æ‰¹é‡è¾“å…¥æ•°æ®
        if text_pair:
            print(f"æ£€æµ‹åˆ°æ–‡æœ¬å¯¹è¾“å…¥ï¼Œå°†textå’Œtext_pairåŒ…è£…æˆå…ƒç»„åˆ—è¡¨ã€‚texté•¿åº¦: {len(text)}, text_pairé•¿åº¦: {len(text_pair)}")
            batched_input = [(text, text_pair)]
        else:
            print(f"å•æ–‡æœ¬è¾“å…¥æ¨¡å¼ï¼Œå°†textåŒ…è£…æˆå•å…ƒç´ åˆ—è¡¨ã€‚texté•¿åº¦: {len(text)}")
            batched_input = [text]
        
        print("\nå¼€å§‹è°ƒç”¨æ ¸å¿ƒç¼–ç æ–¹æ³•_batch_encode_plusï¼Œå‚æ•°è¯¦æƒ…:")
        print(f"â€¢ å¡«å……ç­–ç•¥: {padding_strategy.name}")          # ä¾‹å¦‚ PaddingStrategy.LONGEST
        print(f"â€¢ æˆªæ–­ç­–ç•¥: {truncation_strategy.name}")       # ä¾‹å¦‚ TruncationStrategy.LONGEST_FIRST
        print(f"â€¢ æœ€å¤§é•¿åº¦: {max_length} | æ­¥é•¿: {stride}")
        print(f"â€¢ è¿”å›å¼ é‡ç±»å‹: {return_tensors}")
        print(f"â€¢ ç‰¹æ®Štokenæ‹†åˆ†: {split_special_tokens}")
        
        batched_output = self._batch_encode_plus(
            batched_input,
            is_split_into_words=is_split_into_words,
            add_special_tokens=add_special_tokens,
            padding_strategy=padding_strategy,
            truncation_strategy=truncation_strategy,
            max_length=max_length,
            stride=stride,
            pad_to_multiple_of=pad_to_multiple_of,
            padding_side=padding_side,
            return_tensors=return_tensors,
            return_token_type_ids=return_token_type_ids,
            return_attention_mask=return_attention_mask,
            return_overflowing_tokens=return_overflowing_tokens,
            return_special_tokens_mask=return_special_tokens_mask,
            return_offsets_mapping=return_offsets_mapping,
            return_length=return_length,
            verbose=verbose,
            split_special_tokens=split_special_tokens,
            **kwargs,
        )
        print("\n_batch_encode_plusæ‰§è¡Œå®Œæˆï¼Œè¾“å‡ºåŒ…å«ä»¥ä¸‹é”®:", batched_output.keys())
        
        # print("\nåŠ å¯†ä¸­=========")  # åŸè°ƒè¯•ä¿¡æ¯ä¿ç•™
        
        # åå¤„ç†é€»è¾‘
        if return_tensors is None and not return_overflowing_tokens:
            print("\nè¿›å…¥åå¤„ç†åˆ†æ”¯ï¼šreturn_tensors=Noneä¸”ä¸éœ€è¦æº¢å‡ºtoken")
            print("æ­£åœ¨ç§»é™¤æ‰¹æ¬¡ç»´åº¦ï¼Œæ£€æŸ¥é¦–å…ƒç´ ç±»å‹...")
            
            processed_data = {
                key: (value[0] if len(value) > 0 and isinstance(value[0], list) else value)
                for key, value in batched_output.items()
            }
            print("ç»´åº¦å¤„ç†åæ•°æ®æ ·ä¾‹:", {k: v[:1] for k, v in processed_data.items()})  # æ˜¾ç¤ºéƒ¨åˆ†æ•°æ®
            
            batched_output = BatchEncoding(processed_data, batched_output.encodings)
            print("å·²é‡æ–°å°è£…ä¸ºBatchEncodingå¯¹è±¡")
        else:
            print("\nè·³è¿‡ç»´åº¦å¤„ç†ï¼šéœ€è¦è¿”å›å¼ é‡æˆ–åŒ…å«æº¢å‡ºtoken")
        
        # é•¿åº¦è­¦å‘Šæ£€æŸ¥
        print("\næ­£åœ¨æ£€æŸ¥è¾“å…¥åºåˆ—é•¿åº¦æ˜¯å¦è¶…è¿‡æ¨¡å‹é™åˆ¶...")
        self._eventual_warn_about_too_long_sequence(batched_output["input_ids"], max_length, verbose)
        
        print("\nå¤„ç†å®Œæˆï¼Œè¿”å›æœ€ç»ˆç¼–ç ç»“æœ")
        return batched_output


    def convert_tokens_to_string(self, tokens: List[str]) -> str:
        return (
            self.backend_tokenizer.decoder.decode(tokens)
            if self.backend_tokenizer.decoder is not None
            else " ".join(tokens)
        )

    def _decode(
        self,
        token_ids: Union[int, List[int]],
        skip_special_tokens: bool = False,
        clean_up_tokenization_spaces: bool = None,
        **kwargs,
    ) -> str:
        self._decode_use_source_tokenizer = kwargs.pop("use_source_tokenizer", False)

        if isinstance(token_ids, int):
            token_ids = [token_ids]
        text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)

        clean_up_tokenization_spaces = (
            clean_up_tokenization_spaces
            if clean_up_tokenization_spaces is not None
            else self.clean_up_tokenization_spaces
        )
        if clean_up_tokenization_spaces:
            clean_text = self.clean_up_tokenization(text)
            return clean_text
        else:
            return text

    def _save_pretrained(
        self,
        save_directory: Union[str, os.PathLike],
        file_names: Tuple[str],
        legacy_format: Optional[bool] = None,
        filename_prefix: Optional[str] = None,
    ) -> Tuple[str]:
        """
        Save a tokenizer using the slow-tokenizer/legacy format: vocabulary + added tokens as well as in a unique JSON
        file containing {config + vocab + added-tokens}.
        """
        save_directory = str(save_directory)

        if self.slow_tokenizer_class is None and legacy_format is True:
            raise ValueError(
                "Your tokenizer does not have a legacy version defined and therefore cannot register this version. You"
                " might consider leaving the legacy_format at `None` or setting it to `False`."
            )

        save_slow = (
            (legacy_format is None or legacy_format is True)
            and self.slow_tokenizer_class is not None
            and self.can_save_slow_tokenizer
        )
        save_fast = legacy_format is None or legacy_format is False

        if save_slow:
            added_tokens_file = os.path.join(
                save_directory, (filename_prefix + "-" if filename_prefix else "") + ADDED_TOKENS_FILE
            )
            # make sure to be foward compatible
            added_vocab = {tok: index for tok, index in self.added_tokens_encoder.items() if index >= self.vocab_size}
            if added_vocab:
                with open(added_tokens_file, "w", encoding="utf-8") as f:
                    out_str = json.dumps(added_vocab, indent=2, sort_keys=True, ensure_ascii=False) + "\n"
                    f.write(out_str)

            vocab_files = self.save_vocabulary(save_directory, filename_prefix=filename_prefix)
            file_names = file_names + vocab_files + (added_tokens_file,)

        if save_fast:
            tokenizer_file = os.path.join(
                save_directory, (filename_prefix + "-" if filename_prefix else "") + TOKENIZER_FILE
            )
            self.backend_tokenizer.save(tokenizer_file)
            file_names = file_names + (tokenizer_file,)

        return file_names

    def train_new_from_iterator(
        self,
        text_iterator,
        vocab_size,
        length=None,
        new_special_tokens=None,
        special_tokens_map=None,
        **kwargs,
    ):
        """
        Trains a tokenizer on a new corpus with the same defaults (in terms of special tokens or tokenization pipeline)
        as the current one.

        Args:
            text_iterator (generator of `List[str]`):
                The training corpus. Should be a generator of batches of texts, for instance a list of lists of texts
                if you have everything in memory.
            vocab_size (`int`):
                The size of the vocabulary you want for your tokenizer.
            length (`int`, *optional*):
                The total number of sequences in the iterator. This is used to provide meaningful progress tracking
            new_special_tokens (list of `str` or `AddedToken`, *optional*):
                A list of new special tokens to add to the tokenizer you are training.
            special_tokens_map (`Dict[str, str]`, *optional*):
                If you want to rename some of the special tokens this tokenizer uses, pass along a mapping old special
                token name to new special token name in this argument.
            kwargs (`Dict[str, Any]`, *optional*):
                Additional keyword arguments passed along to the trainer from the ğŸ¤— Tokenizers library.

        Returns:
            [`PreTrainedTokenizerFast`]: A new tokenizer of the same type as the original one, trained on
            `text_iterator`.

        """
        tokenizer_json = json.loads(self._tokenizer.to_str())
        # Remove added tokens for now (uses IDs of tokens)
        added_tokens = tokenizer_json.pop("added_tokens")
        # Remove post processor for now (uses IDs of tokens)
        post_processor = tokenizer_json.pop("post_processor")

        unk_token = None
        # Remove vocab
        if tokenizer_json["model"]["type"] == "BPE":
            tokenizer_json["model"]["vocab"] = {}
            tokenizer_json["model"]["merges"] = []
        elif tokenizer_json["model"]["type"] == "Unigram":
            if tokenizer_json["model"]["unk_id"] is not None:
                unk_id = tokenizer_json["model"]["unk_id"]
                unk_token = tokenizer_json["model"]["vocab"][unk_id][0]
                if special_tokens_map is not None and unk_token in special_tokens_map:
                    unk_token = special_tokens_map[unk_token]
                tokenizer_json["model"]["unk_id"] = 0
                tokenizer_json["model"]["vocab"] = [[unk_token, 0.0]]
        elif tokenizer_json["model"]["type"] in ["WordLevel", "WordPiece"]:
            tokenizer_json["model"]["vocab"] = {}
        else:
            raise ValueError(
                f"This method does not support this type of tokenizer (found {tokenizer_json['model']['type']}) "
                "only BPE, Unigram, WordLevel and WordPiece."
            )

        if (
            special_tokens_map is not None
            and "unk_token" in tokenizer_json["model"]
            and tokenizer_json["model"]["unk_token"] in special_tokens_map
        ):
            tokenizer_json["model"]["unk_token"] = special_tokens_map[tokenizer_json["model"]["unk_token"]]

        tokenizer = TokenizerFast.from_str(json.dumps(tokenizer_json))

        # Get the special tokens from the current tokenizer if none are specified.
        special_tokens = []
        for added_token in added_tokens:
            special = added_token.pop("special", None)
            _ = added_token.pop("id", None)
            if tokenizer_json["model"]["type"] != "Unigram" and not special:
                continue
            if special_tokens_map is not None and added_token["content"] in special_tokens_map:
                added_token["content"] = special_tokens_map[added_token["content"]]
            special_tokens.append(AddedToken(**added_token))

        if new_special_tokens is not None:
            special_tokens.extend(new_special_tokens)

        # Trainer needs to know the end of word / continuing subword thingies in BPE
        if (
            tokenizer_json["model"]["type"] == "BPE"
            and "continuing_subword_prefix" not in kwargs
            and tokenizer_json["model"]["continuing_subword_prefix"] is not None
        ):
            kwargs["continuing_subword_prefix"] = tokenizer_json["model"]["continuing_subword_prefix"]
        if (
            tokenizer_json["model"]["type"] == "BPE"
            and "end_of_word_suffix" not in kwargs
            and tokenizer_json["model"]["end_of_word_suffix"] is not None
        ):
            kwargs["end_of_word_suffix"] = tokenizer_json["model"]["end_of_word_suffix"]
        if tokenizer_json["model"]["type"] == "Unigram" and unk_token is not None:
            kwargs["unk_token"] = unk_token
        if tokenizer_json["pre_tokenizer"] is not None:
            if (
                tokenizer_json["pre_tokenizer"]["type"] == "ByteLevel"
                or tokenizer_json["pre_tokenizer"]["type"] == "Sequence"
                and "pretokenizers" in tokenizer_json["pre_tokenizer"]
                and any(
                    pretokenizer["type"] == "ByteLevel"
                    for pretokenizer in tokenizer_json["pre_tokenizer"]["pretokenizers"]
                )
            ):
                kwargs["initial_alphabet"] = pre_tokenizers_fast.ByteLevel.alphabet()

        trainer_class = MODEL_TO_TRAINER_MAPPING[tokenizer_json["model"]["type"]]
        trainer = trainer_class(vocab_size=vocab_size, special_tokens=special_tokens, **kwargs)
        tokenizer.train_from_iterator(text_iterator, length=length, trainer=trainer)

        if post_processor is not None:
            trained_tokenizer_json = json.loads(tokenizer.to_str())
            # Almost done, we just have to adjust the token IDs in the post processor
            if "special_tokens" in post_processor:
                for key in post_processor["special_tokens"]:
                    tokens = post_processor["special_tokens"][key]["tokens"]
                    if special_tokens_map is not None:
                        tokens = [special_tokens_map.get(token, token) for token in tokens]
                    post_processor["special_tokens"][key]["tokens"] = tokens
                    for token in tokens:
                        token_id = tokenizer.token_to_id(token)
                        if token_id is None:
                            raise ValueError(
                                "Attempted to set a token in the post processor that does not exist in the mapping"
                            )

                    post_processor["special_tokens"][key]["ids"] = [tokenizer.token_to_id(token) for token in tokens]

            for special_token in ["cls", "sep"]:
                if special_token in post_processor:
                    token, _ = post_processor[special_token]
                    if special_tokens_map is not None and token in special_tokens_map:
                        token = special_tokens_map[token]
                    token_id = tokenizer.token_to_id(token)
                    if token_id is None:
                        raise ValueError(
                            "Attempted to set a token in the post processor that does not exist in the mapping"
                        )
                    post_processor[special_token] = [token, token_id]

            trained_tokenizer_json["post_processor"] = post_processor
            tokenizer = TokenizerFast.from_str(json.dumps(trained_tokenizer_json))

        kwargs = self.init_kwargs.copy()
        # Map pad/cls/mask token at the Transformers level
        special_tokens_list = SpecialTokensMixin.SPECIAL_TOKENS_ATTRIBUTES.copy()
        special_tokens_list.remove("additional_special_tokens")
        for token in special_tokens_list:
            if getattr(self, token) is not None:
                special_token = getattr(self, token)
                if special_tokens_map is not None and special_token in special_tokens_map:
                    special_token = special_tokens_map[special_token]

                special_token_full = self._special_tokens_map.get(token, None)
                if isinstance(special_token_full, AddedToken):
                    # Create an added token with the same parameters except the content
                    kwargs[token] = AddedToken(
                        special_token,
                        single_word=special_token_full.single_word,
                        lstrip=special_token_full.lstrip,
                        rstrip=special_token_full.rstrip,
                        normalized=special_token_full.normalized,
                        special=True,
                    )
                else:
                    kwargs[token] = special_token

        additional_special_tokens = self.additional_special_tokens
        if new_special_tokens is not None:
            additional_special_tokens.extend(new_special_tokens)
        if len(additional_special_tokens) > 0:
            kwargs["additional_special_tokens"] = additional_special_tokens

        return self.__class__(tokenizer_object=tokenizer, **kwargs)
